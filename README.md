# Purpose
The purpose of this project is to create an NPC pirate character which finds a hidden treasure before a human player in a maze. I have written the part of the code which represents the intelligent agent, a pirate in this case. The pirate will try to find the optimal path to the treasure using deep Q-learning. Two additional classes (not included here) were used to build the game's framework. The first class, TreasureMaze.py, represents the environment, which includes a maze object defined as a matrix. The second class, GameExperience.py, stores the episodes â€“ that is, all the states that come in between the initial state and the terminal state. This is later used by the agent for learning by experience. The goal of the deep Q-learning implementation is to find the best possible navigation sequence that results in reaching the treasure cell while maximizing the reward. In the implementation, the agent attempts to determine the optimal number of epochs to achieve a 100% win rate.

# Approach
During both exploration and exploitation by the pirate, reinforcement learning is used to help him find the treasure by giving him a consequence each time he takes an action. Because the pirate agent is designed with the goal of maximizing his reward, he will re-create actions which previously resulted in him being rewarded. For example, if the pirate enters a non-empty square, he is penalized, but if he enters an empty square, he is rewarded. The pirate is penalized slightly for each move and rewarded greatly when he reaches the treasure. Because the pirate has a memory which stores his actions and their corresponding states, he can update his policy based on how he is rewarded. This system creates an agent which, through reinforcement, finds the most efficient path to the end goal. 

Reinforcement learning was introduced to the pirate agent using deep Q-learning with a neural network. Instead of a traditional Q-table, a neural network approximates the Q-value function. According to Gulli & Pal, Q-learning applies a stochastic gradient descent to the Bellman equation, propagating rewards backward through the state space and averaging over many trials to improve predictions (2017). The pirate agent's neural network comprises an input layer, five hidden layers, and an output layer. This deep architecture captures complex relationships between states and actions, enabling the agent to learn optimal policies for pathfinding. The Q-Training algorithm uses this network to approximate Q-values, which guides the agent's decisions through the maze. Additionally, the experience replay buffer stores state-action-reward transitions, allowing the network to train on diverse episodes, stabilize learning, and iteratively refine the agent's behavior. Over time, the agent learns to maximize rewards and efficiently navigate to the treasure

# Outcome
This photo depicts the maze environment (black) alongside the agent's optimal path (grey). The number of epochs taken to reach a 100% win rate depends on several factors, though mainly on the exploration value Epsilon. The quickest solution convergence had been 14 minutes with an epsilon value of 0.5, meaning the agent spent half of the time on exploration and half on exploitation. The photo below was taken from a case with epsilon set to 0.1, which converged in approximately 20 minutes.

![adf34c4b-9894-4805-8eeb-015b56dff363](https://github.com/user-attachments/assets/402469b2-de94-4e39-a59d-74513e613b0e)
